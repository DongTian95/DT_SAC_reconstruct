wandb_version: 1

optimizer:
  desc: null
  value: Adam
learning_rate:
  desc: null
  value: 0.0003
batch_size:
  desc: null
  value: 64
plan_length:
  desc: null
  value: 0
epsilon_threshold:
  desc: null
  value: 0.1
eta:
  desc: null
  value: 0.002
alpha:
  desc: null
  value: 0.001
gamma:
  desc: null
  value: 0.98
reward_scale:
  desc: null
  value: 1.0
logging_frequency:
  desc: null
  value: 1000
max_steps:
  desc: null
  value: 101000
mu_clip:
  desc: null
  value: true
commentary:
  desc: null
  value: clip of log_sigma is established and set to (-0.1, 30 * (env.action_high
    - env.action_low)) sigmoid clip of mu is add and set to 3 * (env.action_low, env.action_high)
    the way to get sigma is optimized. Trying out all the learning_rate and alpha
    values here.
hidden_layer_size:
  desc: null
  value: 256
hidden_layer_number:
  desc: null
  value: 2
device:
  desc: null
  value: cuda
random_seed:
  desc: null
  value: 0
activation_function:
  desc: null
  value: ReLU
_wandb:
  desc: null
  value:
    python_version: 3.11.5
    cli_version: 0.16.0
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1701178075.517346
    t:
      1:
      - 1
      - 55
      2:
      - 1
      - 55
      3:
      - 16
      - 23
      4: 3.11.5
      5: 0.16.0
      8:
      - 5
      13: linux-x86_64
